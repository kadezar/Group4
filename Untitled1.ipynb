{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01904ae9-1998-4d9c-8783-76d7e5b0370c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding\n",
    "\n",
    "INITIAL_BALANCE = 1000\n",
    "NUM_DECKS = 6\n",
    "\n",
    "class BlackjackEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(BlackjackEnv, self).__init__()\n",
    "        \n",
    "        # Initialize the blackjack deck.\n",
    "        self.bj_deck = Deck(NUM_DECKS)\n",
    "        \n",
    "        self.player_hand = []\n",
    "        self.dealer_hand = []\n",
    "        \n",
    "        self.reward_options = {\"lose\":-100, \"tie\":0, \"win\":100}\n",
    "        \n",
    "        # hit = 0, stand = 1\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        \n",
    "        '''\n",
    "        First element of tuple is the range of possible hand values for the player. (3 through 20)\n",
    "        This is the possible range of values that the player will actually have to make a decision for.\n",
    "        Any player hand value 21 or above already has automatic valuations, and needs no input from an\n",
    "        AI Agent. \n",
    "        \n",
    "        However, we also need to add all the hand values that the agent could possibly end up in when\n",
    "        they bust. Maybe the agent can glean some correlations based on what hand value they bust at,\n",
    "        so this should be in the observation space. Also, the layout of OpenAI Gym environment class\n",
    "        makes us have to include the bust-value in the step() function because we need to return that\n",
    "        done is true alongside the final obs, which is the bust-value.\n",
    "        '''\n",
    "        \n",
    "        # Second element of the tuple is the range of possible values for the dealer's upcard. (2 through 11)\n",
    "        self.observation_space = spaces.Tuple((spaces.Discrete(18), spaces.Discrete(10)))\n",
    "        \n",
    "        self.done = False\n",
    "        \n",
    "    def _take_action(self, action):\n",
    "        if action == 0: # hit\n",
    "            self.player_hand.append(self.bj_deck.deal())\n",
    "            \n",
    "        # re-calculate the value of the player's hand after any changes to the hand.\n",
    "        self.player_value = player_eval(self.player_hand)\n",
    "    \n",
    "    def step(self, action):\n",
    "        self._take_action(action)\n",
    "        \n",
    "        # End the episode/game is the player stands or has a hand value >= 21.\n",
    "        self.done = action == 1 or self.player_value >= 21\n",
    "        \n",
    "        # rewards are 0 when the player hits and is still below 21, and they\n",
    "        # keep playing.\n",
    "        rewards = 0\n",
    "        \n",
    "        if self.done:\n",
    "            # CALCULATE REWARDS\n",
    "            if self.player_value > 21: # above 21, player loses automatically.\n",
    "                rewards = self.reward_options[\"lose\"]\n",
    "            elif self.player_value == 21: # blackjack! Player wins automatically.\n",
    "                rewards = self.reward_options[\"win\"]\n",
    "            else:\n",
    "                ## Begin dealer turn phase.\n",
    "\n",
    "                dealer_value, self.dealer_hand, self.bj_deck = dealer_turn(self.dealer_hand, self.bj_deck)\n",
    "\n",
    "                ## End of dealer turn phase\n",
    "\n",
    "                #------------------------------------------------------------#\n",
    "\n",
    "                ## Final Compare\n",
    "\n",
    "                if dealer_value > 21: # dealer above 21, player wins automatically\n",
    "                    rewards = self.reward_options[\"win\"]\n",
    "                elif dealer_value == 21: # dealer has blackjack, player loses automatically\n",
    "                    rewards = self.reward_options[\"lose\"]\n",
    "                else: # dealer and player have values less than 21.\n",
    "                    if self.player_value > dealer_value: # player closer to 21, player wins.\n",
    "                        rewards = self.reward_options[\"win\"]\n",
    "                    elif self.player_value < dealer_value: # dealer closer to 21, dealer wins.\n",
    "                        rewards = self.reward_options[\"lose\"]\n",
    "                    else:\n",
    "                        rewards = self.reward_options[\"tie\"]\n",
    "        \n",
    "        self.balance += rewards\n",
    "        \n",
    "        \n",
    "        # Subtract by 1 to fit into the possible observation range.\n",
    "        # This makes the possible range of 3 through 20 into 1 through 18\n",
    "        player_value_obs = self.player_value - 2\n",
    "        \n",
    "        # get the value of the dealer's upcard, this value is what the agent sees.\n",
    "        # Subtract by 1 to fit the possible observation range of 1 to 10.\n",
    "        upcard_value_obs = dealer_eval([self.dealer_upcard]) - 1\n",
    "        \n",
    "        # the state is represented as a player hand-value + dealer upcard pair.\n",
    "        obs = np.array([player_value_obs, upcard_value_obs])\n",
    "        \n",
    "        return obs, rewards, self.done, {}\n",
    "    \n",
    "    def reset(self): # resets game to an initial state\n",
    "        # Add the player and dealer cards back into the deck.\n",
    "        self.bj_deck.cards += self.player_hand + self.dealer_hand\n",
    "\n",
    "        # Shuffle before beginning. Only shuffle once before the start of each game.\n",
    "        self.bj_deck.shuffle()\n",
    "         \n",
    "        self.balance = INITIAL_BALANCE\n",
    "        \n",
    "        self.done = False\n",
    "        \n",
    "        # returns the start state for the agent\n",
    "        # deal 2 cards to the agent and the dealer\n",
    "        self.player_hand = [self.bj_deck.deal(), self.bj_deck.deal()]\n",
    "        self.dealer_hand = [self.bj_deck.deal(), self.bj_deck.deal()]\n",
    "        self.dealer_upcard = self.dealer_hand[0]\n",
    "        \n",
    "        # calculate the value of the agent's hand\n",
    "        self.player_value = player_eval(self.player_hand)\n",
    "        \n",
    "        # Subtract by 1 to fit into the possible observation range.\n",
    "        # This makes the possible range of 2 through 20 into 1 through 18\n",
    "        player_value_obs = self.player_value - 2\n",
    "            \n",
    "        # get the value of the dealer's upcard, this value is what the agent sees.\n",
    "        # Subtract by 1 to fit the possible observation range of 1 to 10.\n",
    "        upcard_value_obs = dealer_eval([self.dealer_upcard]) - 1\n",
    "        \n",
    "        # the state is represented as a player hand-value + dealer upcard pair.\n",
    "        obs = np.array([player_value_obs, upcard_value_obs])\n",
    "        \n",
    "        return obs\n",
    "    \n",
    "    def render(self, mode='human', close=False):\n",
    "        # convert the player hand into a format that is\n",
    "        # easy to read and understand.\n",
    "        hand_list = []\n",
    "        for card in self.player_hand:\n",
    "            hand_list.append(card.rank)\n",
    "            \n",
    "        # re-calculate the value of the dealer upcard.\n",
    "        upcard_value = dealer_eval([self.dealer_upcard])\n",
    "        \n",
    "        print(f'Balance: {self.balance}')\n",
    "        print(f'Player Hand: {hand_list}')\n",
    "        print(f'Player Value: {self.player_value}')\n",
    "        print(f'Dealer Upcard: {upcard_value}')\n",
    "        print(f'Done: {self.done}')\n",
    "        \n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7157a08b-df51-4e56-95ce-8071b8fec6f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Deck' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25812/3439793374.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBlackjackEnv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtotal_rewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mNUM_EPISODES\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25812/1591459401.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;31m# Initialize the blackjack deck.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbj_deck\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDeck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNUM_DECKS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplayer_hand\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Deck' is not defined"
     ]
    }
   ],
   "source": [
    "env = BlackjackEnv()\n",
    "\n",
    "total_rewards = 0\n",
    "NUM_EPISODES = 1000\n",
    "\n",
    "for _ in range(NUM_EPISODES):\n",
    "    env.reset()\n",
    "    \n",
    "    episode_reward = 0\n",
    "    while env.done == False:\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        new_state, reward, done, desc = env.step(action)\n",
    "        \n",
    "        episode_reward += reward\n",
    "        \n",
    "    total_rewards += episode_reward\n",
    "        \n",
    "avg_reward = total_rewards / NUM_EPISODES\n",
    "print(avg_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f23ac7-74ca-4a53-b1d2-303299759882",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
