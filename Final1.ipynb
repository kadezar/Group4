{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d88e9612-e945-4300-a187-09db9bb7a8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8cd7b4f3-f640-4d5f-9a6d-f22e1bae38e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "\n",
    "ranks = {\n",
    "    \"two\" : 2,\n",
    "    \"three\" : 3,\n",
    "    \"four\" : 4,\n",
    "    \"five\" : 5,\n",
    "    \"six\" : 6,\n",
    "    \"seven\" : 7,\n",
    "    \"eight\" : 8,\n",
    "    \"nine\" : 9,\n",
    "    \"ten\" : 10,\n",
    "    \"jack\" : 10,\n",
    "    \"queen\" : 10,\n",
    "    \"king\" : 10,\n",
    "    \"ace\" : (1, 11)\n",
    "}\n",
    "    \n",
    "class Suit(enum.Enum):\n",
    "    spades = \"spades\"\n",
    "    clubs = \"clubs\"\n",
    "    diamonds = \"diamonds\"\n",
    "    hearts = \"hearts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "32449137-b143-40f2-b1c8-dceac246a150",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Card:\n",
    "    def __init__(self, suit, rank, value):\n",
    "        self.suit = suit\n",
    "        self.rank = rank\n",
    "        self.value = value\n",
    "        \n",
    "    def __str__(self):\n",
    "        return self.rank + \" of \" + self.suit.value\n",
    "\n",
    "class Deck:\n",
    "    def __init__(self, num=1):\n",
    "        self.cards = []\n",
    "        for i in range(num):\n",
    "            for suit in Suit:\n",
    "                for rank, value in ranks.items():\n",
    "                    self.cards.append(Card(suit, rank, value))\n",
    "                \n",
    "    def shuffle(self):\n",
    "        random.shuffle(self.cards)\n",
    "        \n",
    "    def deal(self):\n",
    "        return self.cards.pop(0)\n",
    "    \n",
    "    def peek(self):\n",
    "        if len(self.cards) > 0:\n",
    "            return self.cards[0]\n",
    "        \n",
    "    def add_to_bottom(self, card):\n",
    "        self.cards.append(card)\n",
    "        \n",
    "    def __str__(self):\n",
    "        result = \"\"\n",
    "        for card in self.cards:\n",
    "            result += str(card) + \"\\n\"\n",
    "        return result\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.cards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87baa21f-c766-458a-ba94-77a3c9dca34b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689f346d-c22e-4d20-8050-6634bd607370",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340263e7-e874-463b-b928-d574ae716e8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502eb33f-eb56-4d1e-bfde-3d956b998f25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "56537a54-cd7f-4c04-b0c6-960966fb5a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding\n",
    "\n",
    "INITIAL_BALANCE = 1000\n",
    "NUM_DECKS = 6\n",
    "\n",
    "class BlackjackEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(BlackjackEnv, self).__init__()\n",
    "        \n",
    "        # Initialize the blackjack deck.\n",
    "        self.bj_deck = Deck(NUM_DECKS)\n",
    "        \n",
    "        self.player_hand = []\n",
    "        self.dealer_hand = []\n",
    "        \n",
    "        self.reward_options = {\"lose\":-100, \"tie\":0, \"win\":100}\n",
    "        \n",
    "        # hit = 0, stand = 1\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        \n",
    "        # Second element of the tuple is the range of possible values for the dealer's upcard. (2 through 11)\n",
    "        self.observation_space = spaces.Tuple((spaces.Discrete(18), spaces.Discrete(10)))\n",
    "        \n",
    "        self.done = False\n",
    "        \n",
    "    def _take_action(self, action):\n",
    "        if action == 0: # hit\n",
    "            self.player_hand.append(self.bj_deck.deal())\n",
    "            \n",
    "        # re-calculate the value of the player's hand after any changes to the hand.\n",
    "        self.player_value = player_eval(self.player_hand)\n",
    "    \n",
    "    def step(self, action):\n",
    "        self._take_action(action)\n",
    "        \n",
    "        # End the episode/game is the player stands or has a hand value >= 21.\n",
    "        self.done = action == 1 or self.player_value >= 21\n",
    "        rewards = 0\n",
    "        \n",
    "        if self.done:\n",
    "            # CALCULATE REWARDS\n",
    "            if self.player_value > 21: \n",
    "                rewards = self.reward_options[\"lose\"]\n",
    "            elif self.player_value == 21:\n",
    "                rewards = self.reward_options[\"win\"]\n",
    "            else:\n",
    "                dealer_value, self.dealer_hand, self.bj_deck = dealer_turn(self.dealer_hand, self.bj_deck)\n",
    "                if dealer_value > 21: # dealer above 21, player wins automatically\n",
    "                    rewards = self.reward_options[\"win\"]\n",
    "                elif dealer_value == 21: # dealer has blackjack, player loses automatically\n",
    "                    rewards = self.reward_options[\"lose\"]\n",
    "                else: # dealer and player have values less than 21.\n",
    "                    if self.player_value > dealer_value: # player closer to 21, player wins.\n",
    "                        rewards = self.reward_options[\"win\"]\n",
    "                    elif self.player_value < dealer_value: # dealer closer to 21, dealer wins.\n",
    "                        rewards = self.reward_options[\"lose\"]\n",
    "                    else:\n",
    "                        rewards = self.reward_options[\"tie\"]\n",
    "        \n",
    "        self.balance += rewards\n",
    "        \n",
    "        \n",
    "        # Subtract by 1 to fit into the possible observation range.\n",
    "        # This makes the possible range of 3 through 20 into 1 through 18\n",
    "        player_value_obs = self.player_value - 2\n",
    "        \n",
    "        # get the value of the dealer's upcard, this value is what the agent sees.\n",
    "        # Subtract by 1 to fit the possible observation range of 1 to 10.\n",
    "        upcard_value_obs = dealer_eval([self.dealer_upcard]) - 1\n",
    "        \n",
    "        # the state is represented as a player hand-value + dealer upcard pair.\n",
    "        obs = np.array([player_value_obs, upcard_value_obs])\n",
    "        \n",
    "        return obs, rewards, self.done, {}\n",
    "    \n",
    "    def reset(self): # resets game to an initial state\n",
    "        # Add the player and dealer cards back into the deck.\n",
    "        self.bj_deck.cards += self.player_hand + self.dealer_hand\n",
    "\n",
    "        # Shuffle before beginning. Only shuffle once before the start of each game.\n",
    "        self.bj_deck.shuffle()\n",
    "         \n",
    "        self.balance = INITIAL_BALANCE\n",
    "        \n",
    "        self.done = False\n",
    "        \n",
    "        # returns the start state for the agent\n",
    "        # deal 2 cards to the agent and the dealer\n",
    "        self.player_hand = [self.bj_deck.deal(), self.bj_deck.deal()]\n",
    "        self.dealer_hand = [self.bj_deck.deal(), self.bj_deck.deal()]\n",
    "        self.dealer_upcard = self.dealer_hand[0]\n",
    "        \n",
    "        # calculate the value of the agent's hand\n",
    "        self.player_value = player_eval(self.player_hand)\n",
    "        \n",
    "        # Subtract by 1 to fit into the possible observation range.\n",
    "        # This makes the possible range of 2 through 20 into 1 through 18\n",
    "        player_value_obs = self.player_value - 2\n",
    "            \n",
    "        # get the value of the dealer's upcard, this value is what the agent sees.\n",
    "        # Subtract by 1 to fit the possible observation range of 1 to 10.\n",
    "        upcard_value_obs = dealer_eval([self.dealer_upcard]) - 1\n",
    "        \n",
    "        # the state is represented as a player hand-value + dealer upcard pair.\n",
    "        obs = np.array([player_value_obs, upcard_value_obs])\n",
    "        \n",
    "        return obs\n",
    "    \n",
    "    def render(self, mode='human', close=False):\n",
    "        # convert the player hand into a format that is\n",
    "        # easy to read and understand.\n",
    "        hand_list = []\n",
    "        for card in self.player_hand:\n",
    "            hand_list.append(card.rank)\n",
    "            \n",
    "        # re-calculate the value of the dealer upcard.\n",
    "        upcard_value = dealer_eval([self.dealer_upcard])\n",
    "        \n",
    "        print(f'Balance: {self.balance}')\n",
    "        print(f'Player Hand: {hand_list}')\n",
    "        print(f'Player Value: {self.player_value}')\n",
    "        print(f'Dealer Upcard: {upcard_value}')\n",
    "        print(f'Done: {self.done}')\n",
    "        \n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "dfdd180f-9b83-4888-b62a-6096811b284a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-27.9\n"
     ]
    }
   ],
   "source": [
    "env = BlackjackEnv()\n",
    "\n",
    "total_rewards = 0\n",
    "NUM_EPISODES = 1000\n",
    "\n",
    "for _ in range(NUM_EPISODES):\n",
    "    env.reset()\n",
    "    \n",
    "    episode_reward = 0\n",
    "    while env.done == False:\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        new_state, reward, done, desc = env.step(action)\n",
    "        \n",
    "        episode_reward += reward\n",
    "        \n",
    "    total_rewards += episode_reward\n",
    "        \n",
    "avg_reward = total_rewards / NUM_EPISODES\n",
    "print(avg_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "64bcd8fb-0243-4ad9-a619-e5392e30d85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mc(env, num_episodes):\n",
    "    Q = np.zeros([env.observation_space[0].n * env.observation_space[1].n, env.action_space.n], dtype=np.float16)\n",
    "\n",
    "\n",
    "    #Probability distributions for each action (hit or stand) given a state.\n",
    "    prob = np.zeros([env.observation_space[0].n * env.observation_space[1].n, env.action_space.n], dtype=np.float16) + 0.5\n",
    "\n",
    "    alpha = 0.001\n",
    "\n",
    "    epsilon = 1\n",
    "    \n",
    "    # The rate by which epsilon will decay over time.\n",
    "    decay = 0.9999\n",
    "    \n",
    "    # The lowest value that epsilon can go to.\n",
    "    epsilon_min = 0.9\n",
    "\n",
    "    # may have to be tweaked.\n",
    "    gamma = 0.8\n",
    "    \n",
    "    for _ in range(num_episodes):\n",
    "        episode = play_game(env, Q, prob)\n",
    "        \n",
    "        epsilon = max(epsilon * decay, epsilon_min)\n",
    "        \n",
    "        Q = update_Q(env, episode, Q, alpha, gamma)\n",
    "        prob = update_prob(env, episode, Q, prob, epsilon)\n",
    "        \n",
    "    return Q, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d7000b51-5149-44b9-aa0d-7b8c95c55da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(env, Q, prob):\n",
    "    # Can contain numerous state->action->reward tuples because a round of \n",
    "    # Blackjack is not always resolved in one turn.\n",
    "    episode = []\n",
    "    \n",
    "    state = env.reset()\n",
    "    \n",
    "    while env.done == False:\n",
    "        if state[0] == 19:\n",
    "            next_state, reward, env.done, info = env.step(1)\n",
    "        else:\n",
    "            Q_state_index = get_Q_state_index(state)\n",
    "            \n",
    "            # Use the index to get the possible actions. Index 0 is hit, index 1 is stand.\n",
    "            best_action = np.argmax(Q[Q_state_index])\n",
    "            \n",
    "            # Go to the prob table to retrieve the probability of this action.\n",
    "            prob_of_best_action = get_prob_of_best_action(env, state, Q, prob)\n",
    "\n",
    "            action_to_take = None\n",
    "\n",
    "            if random.uniform(0,1) < prob_of_best_action: # Take the best action\n",
    "                action_to_take = best_action\n",
    "            else: # Take the other action\n",
    "                action_to_take = 1 if best_action == 0 else 0\n",
    "            \n",
    "            # The agent does the action, and we get the next state, the rewards,\n",
    "            # and whether the game is now done.\n",
    "            next_state, reward, env.done, info = env.step(action_to_take)\n",
    "            \n",
    "            # We now have a state->action->reward sequence we can log\n",
    "            # in `episode`\n",
    "            episode.append((state, action_to_take, reward))\n",
    "            \n",
    "            # update the state for the next decision made by the agent.\n",
    "            state = next_state\n",
    "        \n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c920a195-2cc5-4183-a5f1-a82eff3fc4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_Q(env, episode, Q, alpha, gamma):\n",
    "    step = 0\n",
    "    for state, action, reward in episode:\n",
    "        # calculate the cumulative reward of taking this action in this state.\n",
    "        # Start from the immediate rewards, and use all the rewards from the\n",
    "        # subsequent states. Do not use rewards from previous states.\n",
    "        total_reward = 0\n",
    "        gamma_exp = 0\n",
    "        for curr_step in range(step, len(episode)):\n",
    "            curr_reward = episode[curr_step][2]\n",
    "            total_reward += (gamma ** gamma_exp) * curr_reward\n",
    "            gamma_exp += 1\n",
    "        \n",
    "        # Update the Q-value\n",
    "        Q_state_index = get_Q_state_index(state)\n",
    "        curr_Q_value = Q[Q_state_index][action]\n",
    "        Q[Q_state_index][action] = curr_Q_value + alpha * (total_reward - curr_Q_value)\n",
    "        \n",
    "        # update step to start further down the episode next time.\n",
    "        step += 1\n",
    "        \n",
    "        \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6da483ee-f115-4adf-a4a2-d9e3bb2aad2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_prob(env, episode, Q, prob, epsilon):\n",
    "    for state, action, reward in episode:\n",
    "        # Update the probabilities of the actions that can be taken given the current\n",
    "        # state.\n",
    "        prob = update_prob_of_best_action(env, state, Q, prob, epsilon)\n",
    "        \n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6844dd89-6321-4dda-92cf-d57f32a8e65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a state, derive the corresponding index in the Q-array.\n",
    "# The state is a player hand value + dealer upcard pair.\n",
    "def get_Q_state_index(state):\n",
    "    # the player value is already subtracted by 1 in the env when it returns the state.\n",
    "    # subtract by 1 again to fit with the array indexing that starts at 0\n",
    "    initial_player_value = state[0] - 1\n",
    "    # the upcard value is already subtracted by 1 in the env when it returns the state.\n",
    "    # dealer_upcard will be subtracted by 1 to fit with the array indexing that starts at 0\n",
    "    dealer_upcard = state[1] - 1\n",
    "\n",
    "    return (env.observation_space[1].n * (initial_player_value)) + (dealer_upcard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ee32aeaa-45e4-4b44-a11a-8cc365df59d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob_of_best_action(env, state, Q, prob):\n",
    "    # Use the mapping function to figure out which index of Q corresponds to \n",
    "    # the player hand value + dealer upcard value that defines each state.\n",
    "    Q_state_index = get_Q_state_index(state)\n",
    "    \n",
    "    # Use the np.argmax() function to find the index of the action that yields the\n",
    "    # rewards i.e. the best action we are looking for.\n",
    "    best_action = np.argmax(Q[Q_state_index])\n",
    "    \n",
    "    # Retrieve the probability of the best action using the \n",
    "    # state/action pair as indices for the `prob` array\n",
    "    return prob[Q_state_index][best_action]\n",
    "    \n",
    "def update_prob_of_best_action(env, state, Q, prob, epsilon):\n",
    "\n",
    "    Q_state_index = get_Q_state_index(state)\n",
    "    \n",
    "    best_action = np.argmax(Q[Q_state_index])\n",
    "    \n",
    "    prob[Q_state_index][best_action] = min(1, prob[Q_state_index][best_action] + 1 - epsilon)\n",
    "    \n",
    "    other_action = 1 if best_action == 0 else 0\n",
    "    prob[Q_state_index][other_action] = 1 - prob[Q_state_index][best_action]\n",
    "    \n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7245fd71-1d74-4658-9810-f8d8a165eee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Time for Learning: 235.92281222343445\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "env = BlackjackEnv()\n",
    "\n",
    "start_time = time.time()\n",
    "new_Q, new_prob = run_mc(env, 1000000)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Total Time for Learning: \" + str(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9b5286b9-fbd3-460a-8751-ba98eda66329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_policy(Q):\n",
    "    best_policy_binary = []\n",
    "    best_policy_string = []\n",
    "    best_policy_colors = []\n",
    "    for i in range(len(Q)):\n",
    "        best_policy_binary.append(np.argmax(Q[i]))\n",
    "        best_policy_string.append(\"H\" if np.argmax(Q[i]) == 0 else \"S\")\n",
    "        best_policy_colors.append(\"g\" if np.argmax(Q[i]) == 0 else \"r\")\n",
    "        \n",
    "    return best_policy_binary, best_policy_string, best_policy_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "32b3b285-1e95-4147-9caf-d4035ed23f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "new_Q_binary, new_Q_string, new_Q_colors = best_policy(new_Q)\n",
    "\n",
    "df = pd.DataFrame(columns = range(2, 12))\n",
    "\n",
    "color_df = pd.DataFrame(columns = range(2, 12))\n",
    "\n",
    "for s in range(3, 21): \n",
    "    start = env.observation_space[1].n * (s-3)\n",
    "    end = start + 10\n",
    "    df.loc[s]=(new_Q_string[start:end])\n",
    "    color_df.loc[s]=(new_Q_colors[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "839059df-c948-41b1-8054-ac87e13fc9e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# hide axes\n",
    "fig.patch.set_visible(False)\n",
    "ax.set_axis_off()\n",
    "ax.axis('tight')\n",
    "\n",
    "ax.table(cellText=df.values, cellColours=color_df.values, cellLoc=\"center\", rowLabels=df.index, colLabels=df.columns, loc='center')\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c248a3f0-e187-4a57-9da5-f91b744486ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4.966\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "env = BlackjackEnv()\n",
    "\n",
    "total_rewards = 0\n",
    "NUM_EPISODES = 100000\n",
    "\n",
    "for _ in range(NUM_EPISODES):\n",
    "    state = env.reset()\n",
    "\n",
    "    while env.done == False:\n",
    "        if state[0] == 19: # Player was dealt Blackjack\n",
    "            next_state, reward, env.done, info = env.step(1) # doesn't matter what action is taken.\n",
    "            # don't do any episode analysis for this episode. This is a useless episode.\n",
    "            total_rewards += reward\n",
    "        else:\n",
    "            Q_index = get_Q_state_index(state)\n",
    "            action = new_Q_binary[Q_index]\n",
    "\n",
    "            new_state, reward, done, desc = env.step(action)\n",
    "            state = new_state\n",
    "            total_rewards += reward\n",
    "        \n",
    "avg_reward = total_rewards / NUM_EPISODES\n",
    "print(avg_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200d71f9-1393-4968-9284-568db013facc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919b42db-ff76-4d0e-9907-2f9779015f45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4354ffdc-29e1-45ec-8757-d1b7caebdd14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
