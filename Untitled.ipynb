{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8c5811ff-f5cc-4159-847f-2056718da426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8fb08b33-dc52-4c38-95b8-f3fc7325647c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "CARD_VALUES = {\n",
    "        '2': 1,\n",
    "        '3': 1,\n",
    "        '4': 1,\n",
    "        '5': 1,\n",
    "        '6': 1,\n",
    "        '7': 0,\n",
    "        '8': 0,\n",
    "        '9': 0,\n",
    "        '0': -1, # use '0' for 10 to keep everything a single character\n",
    "        'J': -1,\n",
    "        'Q': -1,\n",
    "        'K': -1,\n",
    "        'A': -1,\n",
    "        '*': -1, # use '*' as an alias for 'A' to make using the number pad easier\n",
    "        }\n",
    "\n",
    "    \n",
    "ranks = {\n",
    "    \"two\" : 2,\n",
    "    \"three\" : 3,\n",
    "    \"four\" : 4,\n",
    "    \"five\" : 5,\n",
    "    \"six\" : 6,\n",
    "    \"seven\" : 7,\n",
    "    \"eight\" : 8,\n",
    "    \"nine\" : 9,\n",
    "    \"ten\" : 10,\n",
    "    \"jack\" : 11,\n",
    "    \"queen\" : 12,\n",
    "    \"king\" : 13,\n",
    "    \"ace\" : (1, 11)\n",
    "}\n",
    "    \n",
    "class Suit(enum.Enum):\n",
    "    spades = \"spades\"\n",
    "    clubs = \"clubs\"\n",
    "    diamonds = \"diamonds\"\n",
    "    hearts = \"hearts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4a6f598a-57f4-4b2f-b618-7829ef7d025d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Card:\n",
    "    def __init__(self, suit, rank, value):\n",
    "        self.suit = suit\n",
    "        self.rank = rank\n",
    "        self.value = value\n",
    "        \n",
    "    def __str__(self):\n",
    "        return self.rank + \" of \" + self.suit.value\n",
    "\n",
    "class Deck:\n",
    "    def __init__(self, num=1):\n",
    "        self.cards = []\n",
    "        for i in range(num):\n",
    "            for suit in Suit:\n",
    "                for rank, value in ranks.items():\n",
    "                    self.cards.append(Card(suit, rank, value))\n",
    "                \n",
    "    def shuffle(self):\n",
    "        random.shuffle(self.cards)\n",
    "        \n",
    "    def deal(self):\n",
    "        return self.cards.pop(0)\n",
    "    \n",
    "    def peek(self):\n",
    "        if len(self.cards) > 0:\n",
    "            return self.cards[0]\n",
    "        \n",
    "    def add_to_bottom(self, card):\n",
    "        self.cards.append(card)\n",
    "        \n",
    "    def __str__(self):\n",
    "        result = \"\"\n",
    "        for card in self.cards:\n",
    "            result += str(card) + \"\\n\"\n",
    "        return result\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.cards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ecab0a-27c6-4287-bd64-2e4443e78eb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5567a18-d42c-4805-9376-4a42afad3031",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18292003-5f12-4dd3-ad83-905e4bed1c91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad62cdf-e431-4515-a1c9-fc69e35edfce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1ae5b8-161c-4d3e-953a-0d2bc2901f16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4ea036-68ea-4599-b7d8-f61b2f26107e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8650b4-b6dc-4d0d-9ae5-e8dbd967de62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e00b84d1-052d-4470-99bb-8decf930f50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding\n",
    "\n",
    "INITIAL_BALANCE = 1000\n",
    "NUM_DECKS = 6\n",
    "\n",
    "class BlackjackEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(BlackjackEnv, self).__init__()\n",
    "        \n",
    "        # Initialize the blackjack deck.\n",
    "        self.bj_deck = Deck(NUM_DECKS)\n",
    "        \n",
    "        self.player_hand = []\n",
    "        self.dealer_hand = []\n",
    "        \n",
    "        self.reward_options = {\"lose\":-100, \"tie\":0, \"win\":100}\n",
    "        \n",
    "        # hit = 0, stand = 1\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        \n",
    "        '''\n",
    "        First element of tuple is the range of possible hand values for the player. (3 through 20)\n",
    "        This is the possible range of values that the player will actually have to make a decision for.\n",
    "        Any player hand value 21 or above already has automatic valuations, and needs no input from an\n",
    "        AI Agent. \n",
    "        \n",
    "        However, we also need to add all the hand values that the agent could possibly end up in when\n",
    "        they bust. Maybe the agent can glean some correlations based on what hand value they bust at,\n",
    "        so this should be in the observation space. Also, the layout of OpenAI Gym environment class\n",
    "        makes us have to include the bust-value in the step() function because we need to return that\n",
    "        done is true alongside the final obs, which is the bust-value.\n",
    "        '''\n",
    "        \n",
    "        # Second element of the tuple is the range of possible values for the dealer's upcard. (2 through 11)\n",
    "        self.observation_space = spaces.Tuple((spaces.Discrete(18), spaces.Discrete(10)))\n",
    "        \n",
    "        self.done = False\n",
    "        \n",
    "    def _take_action(self, action):\n",
    "        if action == 0: # hit\n",
    "            self.player_hand.append(self.bj_deck.deal())\n",
    "            \n",
    "        # re-calculate the value of the player's hand after any changes to the hand.\n",
    "        self.player_value = player_eval(self.player_hand)\n",
    "    \n",
    "    def step(self, action):\n",
    "        self._take_action(action)\n",
    "        \n",
    "        # End the episode/game is the player stands or has a hand value >= 21.\n",
    "        self.done = action == 1 or self.player_value >= 21\n",
    "        \n",
    "        # rewards are 0 when the player hits and is still below 21, and they\n",
    "        # keep playing.\n",
    "        rewards = 0\n",
    "        \n",
    "        if self.done:\n",
    "            # CALCULATE REWARDS\n",
    "            if self.player_value > 21: # above 21, player loses automatically.\n",
    "                rewards = self.reward_options[\"lose\"]\n",
    "            elif self.player_value == 21: # blackjack! Player wins automatically.\n",
    "                rewards = self.reward_options[\"win\"]\n",
    "            else:\n",
    "                ## Begin dealer turn phase.\n",
    "\n",
    "                dealer_value, self.dealer_hand, self.bj_deck = dealer_turn(self.dealer_hand, self.bj_deck)\n",
    "\n",
    "                ## End of dealer turn phase\n",
    "\n",
    "                #------------------------------------------------------------#\n",
    "\n",
    "                ## Final Compare\n",
    "\n",
    "                if dealer_value > 21: # dealer above 21, player wins automatically\n",
    "                    rewards = self.reward_options[\"win\"]\n",
    "                elif dealer_value == 21: # dealer has blackjack, player loses automatically\n",
    "                    rewards = self.reward_options[\"lose\"]\n",
    "                else: # dealer and player have values less than 21.\n",
    "                    if self.player_value > dealer_value: # player closer to 21, player wins.\n",
    "                        rewards = self.reward_options[\"win\"]\n",
    "                    elif self.player_value < dealer_value: # dealer closer to 21, dealer wins.\n",
    "                        rewards = self.reward_options[\"lose\"]\n",
    "                    else:\n",
    "                        rewards = self.reward_options[\"tie\"]\n",
    "        \n",
    "        self.balance += rewards\n",
    "        \n",
    "        \n",
    "        # Subtract by 1 to fit into the possible observation range.\n",
    "        # This makes the possible range of 3 through 20 into 1 through 18\n",
    "        player_value_obs = self.player_value - 2\n",
    "        \n",
    "        # get the value of the dealer's upcard, this value is what the agent sees.\n",
    "        # Subtract by 1 to fit the possible observation range of 1 to 10.\n",
    "        upcard_value_obs = dealer_eval([self.dealer_upcard]) - 1\n",
    "        \n",
    "        # the state is represented as a player hand-value + dealer upcard pair.\n",
    "        obs = np.array([player_value_obs, upcard_value_obs])\n",
    "        \n",
    "        return obs, rewards, self.done, {}\n",
    "    \n",
    "    def reset(self): # resets game to an initial state\n",
    "        # Add the player and dealer cards back into the deck.\n",
    "        self.bj_deck.cards += self.player_hand + self.dealer_hand\n",
    "\n",
    "        # Shuffle before beginning. Only shuffle once before the start of each game.\n",
    "        self.bj_deck.shuffle()\n",
    "         \n",
    "        self.balance = INITIAL_BALANCE\n",
    "        \n",
    "        self.done = False\n",
    "        \n",
    "        # returns the start state for the agent\n",
    "        # deal 2 cards to the agent and the dealer\n",
    "        self.player_hand = [self.bj_deck.deal(), self.bj_deck.deal()]\n",
    "        self.dealer_hand = [self.bj_deck.deal(), self.bj_deck.deal()]\n",
    "        self.dealer_upcard = self.dealer_hand[0]\n",
    "        \n",
    "        # calculate the value of the agent's hand\n",
    "        self.player_value = player_eval(self.player_hand)\n",
    "        \n",
    "        # Subtract by 1 to fit into the possible observation range.\n",
    "        # This makes the possible range of 2 through 20 into 1 through 18\n",
    "        player_value_obs = self.player_value - 2\n",
    "            \n",
    "        # get the value of the dealer's upcard, this value is what the agent sees.\n",
    "        # Subtract by 1 to fit the possible observation range of 1 to 10.\n",
    "        upcard_value_obs = dealer_eval([self.dealer_upcard]) - 1\n",
    "        \n",
    "        # the state is represented as a player hand-value + dealer upcard pair.\n",
    "        obs = np.array([player_value_obs, upcard_value_obs])\n",
    "        \n",
    "        return obs\n",
    "    \n",
    "    def render(self, mode='human', close=False):\n",
    "        # convert the player hand into a format that is\n",
    "        # easy to read and understand.\n",
    "        hand_list = []\n",
    "        for card in self.player_hand:\n",
    "            hand_list.append(card.rank)\n",
    "            \n",
    "        # re-calculate the value of the dealer upcard.\n",
    "        upcard_value = dealer_eval([self.dealer_upcard])\n",
    "        \n",
    "        print(f'Balance: {self.balance}')\n",
    "        print(f'Player Hand: {hand_list}')\n",
    "        print(f'Player Value: {self.player_value}')\n",
    "        print(f'Dealer Upcard: {upcard_value}')\n",
    "        print(f'Done: {self.done}')\n",
    "        \n",
    "        print()\n",
    "        print(self.observation_space)\n",
    "        print(gym.Env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "aec180f9-161c-48ed-a4f5-3bb789ebef94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Env in module gym.core:\n",
      "\n",
      "class Env(typing.Generic)\n",
      " |  Env(*args, **kwds)\n",
      " |  \n",
      " |  The main OpenAI Gym class. It encapsulates an environment with\n",
      " |  arbitrary behind-the-scenes dynamics. An environment can be\n",
      " |  partially or fully observed.\n",
      " |  \n",
      " |  The main API methods that users of this class need to know are:\n",
      " |  \n",
      " |      step\n",
      " |      reset\n",
      " |      render\n",
      " |      close\n",
      " |      seed\n",
      " |  \n",
      " |  And set the following attributes:\n",
      " |  \n",
      " |      action_space: The Space object corresponding to valid actions\n",
      " |      observation_space: The Space object corresponding to valid observations\n",
      " |      reward_range: A tuple corresponding to the min and max possible rewards\n",
      " |  \n",
      " |  Note: a default reward range set to [-inf,+inf] already exists. Set it if you want a narrower range.\n",
      " |  \n",
      " |  The methods are accessed publicly as \"step\", \"reset\", etc...\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Env\n",
      " |      typing.Generic\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __enter__(self)\n",
      " |      Support with-statement for the environment.\n",
      " |  \n",
      " |  __exit__(self, *args)\n",
      " |      Support with-statement for the environment.\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  close(self)\n",
      " |      Override close in your subclass to perform any necessary cleanup.\n",
      " |      \n",
      " |      Environments will automatically close() themselves when\n",
      " |      garbage collected or when the program exits.\n",
      " |  \n",
      " |  render(self, mode='human')\n",
      " |      Renders the environment.\n",
      " |      \n",
      " |      The set of supported modes varies per environment. (And some\n",
      " |      third-party environments may not support rendering at all.)\n",
      " |      By convention, if mode is:\n",
      " |      \n",
      " |      - human: render to the current display or terminal and\n",
      " |        return nothing. Usually for human consumption.\n",
      " |      - rgb_array: Return an numpy.ndarray with shape (x, y, 3),\n",
      " |        representing RGB values for an x-by-y pixel image, suitable\n",
      " |        for turning into a video.\n",
      " |      - ansi: Return a string (str) or StringIO.StringIO containing a\n",
      " |        terminal-style text representation. The text can include newlines\n",
      " |        and ANSI escape sequences (e.g. for colors).\n",
      " |      \n",
      " |      Note:\n",
      " |          Make sure that your class's metadata 'render_modes' key includes\n",
      " |            the list of supported modes. It's recommended to call super()\n",
      " |            in implementations to use the functionality of this method.\n",
      " |      \n",
      " |      Args:\n",
      " |          mode (str): the mode to render with\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      class MyEnv(Env):\n",
      " |          metadata = {'render_modes': ['human', 'rgb_array']}\n",
      " |      \n",
      " |          def render(self, mode='human'):\n",
      " |              if mode == 'rgb_array':\n",
      " |                  return np.array(...) # return RGB frame suitable for video\n",
      " |              elif mode == 'human':\n",
      " |                  ... # pop up a window and render\n",
      " |              else:\n",
      " |                  super(MyEnv, self).render(mode=mode) # just raise an exception\n",
      " |  \n",
      " |  reset(self, *, seed: 'Optional[int]' = None, return_info: 'bool' = False, options: 'Optional[dict]' = None) -> 'Union[ObsType, tuple[ObsType, dict]]'\n",
      " |      Resets the environment to an initial state and returns an initial\n",
      " |      observation.\n",
      " |      \n",
      " |      This method should also reset the environment's random number\n",
      " |      generator(s) if `seed` is an integer or if the environment has not\n",
      " |      yet initialized a random number generator. If the environment already\n",
      " |      has a random number generator and `reset` is called with `seed=None`,\n",
      " |      the RNG should not be reset.\n",
      " |      Moreover, `reset` should (in the typical use case) be called with an\n",
      " |      integer seed right after initialization and then never again.\n",
      " |      \n",
      " |      Returns:\n",
      " |          observation (object): the initial observation.\n",
      " |          info (optional dictionary): a dictionary containing extra information, this is only returned if return_info is set to true\n",
      " |  \n",
      " |  seed(self, seed=None)\n",
      " |      Sets the seed for this env's random number generator(s).\n",
      " |      \n",
      " |      Note:\n",
      " |          Some environments use multiple pseudorandom number generators.\n",
      " |          We want to capture all such seeds used in order to ensure that\n",
      " |          there aren't accidental correlations between multiple generators.\n",
      " |      \n",
      " |      Returns:\n",
      " |          list<bigint>: Returns the list of seeds used in this env's random\n",
      " |            number generators. The first value in the list should be the\n",
      " |            \"main\" seed, or the value which a reproducer should pass to\n",
      " |            'seed'. Often, the main seed equals the provided 'seed', but\n",
      " |            this won't be true if seed=None, for example.\n",
      " |  \n",
      " |  step(self, action: 'ActType') -> 'Tuple[ObsType, float, bool, dict]'\n",
      " |      Run one timestep of the environment's dynamics. When end of\n",
      " |      episode is reached, you are responsible for calling `reset()`\n",
      " |      to reset this environment's state.\n",
      " |      \n",
      " |      Accepts an action and returns a tuple (observation, reward, done, info).\n",
      " |      \n",
      " |      Args:\n",
      " |          action (object): an action provided by the agent\n",
      " |      \n",
      " |      Returns:\n",
      " |          observation (object): agent's observation of the current environment\n",
      " |          reward (float) : amount of reward returned after previous action\n",
      " |          done (bool): whether the episode has ended, in which case further step() calls will return undefined results\n",
      " |          info (dict): contains auxiliary diagnostic information (helpful for debugging, logging, and sometimes learning)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  np_random\n",
      " |      Initializes the np_random field if not done already.\n",
      " |  \n",
      " |  unwrapped\n",
      " |      Completely unwrap this env.\n",
      " |      \n",
      " |      Returns:\n",
      " |          gym.Env: The base non-wrapped gym.Env instance\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {'_np_random': 'RandomNumberGenerator | None', 'acti...\n",
      " |  \n",
      " |  __orig_bases__ = (typing.Generic[~ObsType, ~ActType],)\n",
      " |  \n",
      " |  __parameters__ = (~ObsType, ~ActType)\n",
      " |  \n",
      " |  metadata = {'render_modes': []}\n",
      " |  \n",
      " |  reward_range = (-inf, inf)\n",
      " |  \n",
      " |  spec = None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __class_getitem__(params) from builtins.type\n",
      " |  \n",
      " |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __new__(cls, *args, **kwds)\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "help(gym.Env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2850ec9d-892a-4778-a49a-2856237f00cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-33.3\n"
     ]
    }
   ],
   "source": [
    "env = BlackjackEnv()\n",
    "\n",
    "total_rewards = 0\n",
    "NUM_EPISODES = 1000\n",
    "\n",
    "for _ in range(NUM_EPISODES):\n",
    "    env.reset()\n",
    "    \n",
    "    episode_reward = 0\n",
    "    while env.done == False:\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        new_state, reward, done, desc = env.step(action)\n",
    "        \n",
    "        episode_reward += reward\n",
    "        \n",
    "    total_rewards += episode_reward\n",
    "        \n",
    "avg_reward = total_rewards / NUM_EPISODES\n",
    "print(avg_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "35149cf4-f44e-497d-95dc-43a4df7a6411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    count = 0\n",
    "    cards = 0\n",
    "    user_input = True\n",
    "    decks_played = 0\n",
    "    while user_input:\n",
    "        user_input = input('>> ')\n",
    "        cards += len(user_input)\n",
    "        for card in user_input:\n",
    "            count += CARD_VALUES[card.upper()]\n",
    "        decks_played = cards / 52.0\n",
    "        true_count = count / (DECKS - decks_played)\n",
    "        print('Count: {}'.format(count))\n",
    "        print('True Count: {}'.format(true_count))\n",
    "    print('Decks played: {}'.format(decks_played))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c8e53669-6545-4954-9705-418374c6349f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5af97e32-408b-4cd0-9fce-2c00c8d9e003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mc(env, num_episodes):\n",
    "    '''\n",
    "    observation_space[0] is the 18 possible player values. (3 through 20)\n",
    "    observation_space[1] is the 10 possible dealer upcards. (2 through 11)\n",
    "\n",
    "    Combining these together yields all possible states.\n",
    "\n",
    "    Multiplying this with hit/stand yields all possible state/action pairs.\n",
    "\n",
    "    This is the Q map.\n",
    "    '''\n",
    "    Q = np.zeros([env.observation_space[0].n * env.observation_space[1].n, env.action_space.n], dtype=np.float16)\n",
    "\n",
    "\n",
    "    # This map contains the probability distributions for each action (hit or stand) given a state.\n",
    "    # The state (combo of player hand value and dealer upcard value) index in this array yields a 2-element array\n",
    "    # The 0th index of this 2-element array refers to the probability of \"hit\", and the 1st index is the probability of \"stand\"\n",
    "    prob = np.zeros([env.observation_space[0].n * env.observation_space[1].n, env.action_space.n], dtype=np.float16) + 0.5\n",
    "    # The learning rate. Very small to avoid making quick, large changes in our policy.\n",
    "\n",
    "    alpha = 0.001\n",
    "\n",
    "    epsilon = 1\n",
    "    \n",
    "    # The rate by which epsilon will decay over time.\n",
    "    # Since the probability we take the option with the highest Q-value is 1-epsilon + probability,\n",
    "    # this decay will make sure we are the taking the better option more often in the longrun.\n",
    "    # This allows the algorithm to explore in the early stages, and exploit in the later stages.\n",
    "    decay = 0.9999\n",
    "    \n",
    "    # The lowest value that epsilon can go to.\n",
    "    # Although the decay seems slow, it actually grows exponentially, and this is magnified when\n",
    "    # running thousands of episodes.\n",
    "    epsilon_min = 0.9\n",
    "\n",
    "    # may have to be tweaked later.\n",
    "    gamma = 0.8\n",
    "    \n",
    "    for _ in range(num_episodes):\n",
    "        episode = play_game(env, Q, prob)\n",
    "        \n",
    "        epsilon = max(epsilon * decay, epsilon_min)\n",
    "        \n",
    "        Q = update_Q(env, episode, Q, alpha, gamma)\n",
    "        prob = update_prob(env, episode, Q, prob, epsilon)\n",
    "        \n",
    "    return Q, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "404118e3-b2a1-48d9-b569-136ea272f26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(env, Q, prob):\n",
    "    # Can contain numerous state->action->reward tuples because a round of \n",
    "    # Blackjack is not always resolved in one turn.\n",
    "    # However, there will be no state that has a player hand value that exceeds 20, since only initial\n",
    "    # states BEFORE actions are made are used when storing state->action->reward tuples.\n",
    "    episode = []\n",
    "    \n",
    "    state = env.reset()\n",
    "    \n",
    "    while env.done == False:\n",
    "        if state[0] == 19: #Player was dealt Blackjack, player_value already subtracted by 2 to get state[0]\n",
    "            # don't do any episode analysis for this episode. This is a useless episode.\n",
    "            next_state, reward, env.done, info = env.step(1) # doesn't matter what action is taken.\n",
    "        else:\n",
    "            # Get the index in Q that corresponds to the current state\n",
    "            Q_state_index = get_Q_state_index(state)\n",
    "            \n",
    "            # Use the index to get the possible actions, and use np.argmax()\n",
    "            # to get the index of the action that has the highest current Q\n",
    "            # value. Index 0 is hit, index 1 is stand.\n",
    "            best_action = np.argmax(Q[Q_state_index])\n",
    "            \n",
    "            # Go to the prob table to retrieve the probability of this action.\n",
    "            # This uses the same Q_state_index used for finding the state index\n",
    "            # of the Q-array.\n",
    "            prob_of_best_action = get_prob_of_best_action(env, state, Q, prob)\n",
    "\n",
    "            action_to_take = None\n",
    "\n",
    "            if random.uniform(0,1) < prob_of_best_action: # Take the best action\n",
    "                action_to_take = best_action\n",
    "            else: # Take the other action\n",
    "                action_to_take = 1 if best_action == 0 else 0\n",
    "            \n",
    "            # The agent does the action, and we get the next state, the rewards,\n",
    "            # and whether the game is now done.\n",
    "            next_state, reward, env.done, info = env.step(action_to_take)\n",
    "            \n",
    "            # We now have a state->action->reward sequence we can log\n",
    "            # in `episode`\n",
    "            episode.append((state, action_to_take, reward))\n",
    "            \n",
    "            # update the state for the next decision made by the agent.\n",
    "            state = next_state\n",
    "        \n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "34f3386a-1565-4f62-9f88-9ce45e584364",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_Q(env, episode, Q, alpha, gamma):\n",
    "    '''\n",
    "    THIS IS WHERE THE ALGORITHM HINGES ON BEING FIRST VISIT OR EVERY VISIT.\n",
    "    I AM GOING TO USE FIRST-VISIT, AND HERE'S WHY.\n",
    "    \n",
    "    If you want first-visit, you need to use the cumulative reward of the entire\n",
    "    episode when updating a Q-value for ALL of the state/action pairs in the\n",
    "    episode, even the first state/action pair. In this algorithm, an episode\n",
    "    is a round of Blackjack. Although the bulk of the reward may come from the\n",
    "    2nd or 3rd decision, deciding to hit on the 1st decision is what enabled\n",
    "    the future situations to even occur, so it is important to include the\n",
    "    entire cumulative reward. We can reduce the impact of the rewards of the\n",
    "    future decisions by lowering gamma, which will lower the G value for our\n",
    "    early state/action pair in which we hit and did not get any immediate rewards.\n",
    "    This will make our agent consider future rewards, and not just look at \n",
    "    each state in isolation despite having hit previously.\n",
    "     \n",
    "    If you want Every-Visit MC, do not use the cumulative rewards when updating Q-values,\n",
    "    and just use the immediate reward in this episode for each state/action pair.\n",
    "    '''\n",
    "    step = 0\n",
    "    for state, action, reward in episode:\n",
    "        # calculate the cumulative reward of taking this action in this state.\n",
    "        # Start from the immediate rewards, and use all the rewards from the\n",
    "        # subsequent states. Do not use rewards from previous states.\n",
    "        total_reward = 0\n",
    "        gamma_exp = 0\n",
    "        for curr_step in range(step, len(episode)):\n",
    "            curr_reward = episode[curr_step][2]\n",
    "            total_reward += (gamma ** gamma_exp) * curr_reward\n",
    "            gamma_exp += 1\n",
    "        \n",
    "        # Update the Q-value\n",
    "        Q_state_index = get_Q_state_index(state)\n",
    "        curr_Q_value = Q[Q_state_index][action]\n",
    "        Q[Q_state_index][action] = curr_Q_value + alpha * (total_reward - curr_Q_value)\n",
    "        \n",
    "        # update step to start further down the episode next time.\n",
    "        step += 1\n",
    "        \n",
    "        \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e6ddc512-ac4a-476a-a174-01ae943b5e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_prob(env, episode, Q, prob, epsilon):\n",
    "    for state, action, reward in episode:\n",
    "        # Update the probabilities of the actions that can be taken given the current\n",
    "        # state. The goal is that the new update in Q has changed what the best action\n",
    "        # is, and epsilon will be used to create a small increase in the probability\n",
    "        # that the new, better action is chosen.\n",
    "        prob = update_prob_of_best_action(env, state, Q, prob, epsilon)\n",
    "        \n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "02a40f42-f33b-4371-ae23-ecf953398ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a state, derive the corresponding index in the Q-array.\n",
    "# The state is a player hand value + dealer upcard pair,\n",
    "# so a \"hashing\" formula must be used to allocate the\n",
    "# indices of the Q-array properly.\n",
    "def get_Q_state_index(state):\n",
    "    # the player value is already subtracted by 1 in the env when it returns the state.\n",
    "    # subtract by 1 again to fit with the array indexing that starts at 0\n",
    "    initial_player_value = state[0] - 1\n",
    "    # the upcard value is already subtracted by 1 in the env when it returns the state.\n",
    "    # dealer_upcard will be subtracted by 1 to fit with the array indexing that starts at 0\n",
    "    dealer_upcard = state[1] - 1\n",
    "\n",
    "    return (env.observation_space[1].n * (initial_player_value)) + (dealer_upcard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "83097324-5220-4cd6-9c53-7cec5e0c83ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob_of_best_action(env, state, Q, prob):\n",
    "    # Use the mapping function to figure out which index of Q corresponds to \n",
    "    # the player hand value + dealer upcard value that defines each state.\n",
    "    Q_state_index = get_Q_state_index(state)\n",
    "    \n",
    "    # Use this index in the Q 2-D array to get a 2-element array that yield\n",
    "    # the current Q-values for hitting (index 0) and standing (index 1) in this state.\n",
    "    # Use the np.argmax() function to find the index of the action that yields the\n",
    "    # rewards i.e. the best action we are looking for.\n",
    "    best_action = np.argmax(Q[Q_state_index])\n",
    "    \n",
    "    # Retrieve the probability of the best action using the \n",
    "    # state/action pair as indices for the `prob` array,\n",
    "    # which stores the probability of taking an action (hit or stand)\n",
    "    # for a given state/action pair.\n",
    "    return prob[Q_state_index][best_action]\n",
    "    \n",
    "def update_prob_of_best_action(env, state, Q, prob, epsilon):\n",
    "\n",
    "    Q_state_index = get_Q_state_index(state)\n",
    "    \n",
    "    best_action = np.argmax(Q[Q_state_index])\n",
    "    \n",
    "    # Slightly alter the probability of this best action being taken by using epsilon\n",
    "    # Epsilon starts at 1.0, and slowly decays over time.\n",
    "    # Therefore, as per the equation below, the AI agent will use the probability listed \n",
    "    # for the best action in the `prob` array during the beginning of the algorithm.\n",
    "    # As time goes on, the likelihood that the best action is taken is increased from\n",
    "    # what is listed in the `prob` array.\n",
    "    # This allows for exploration of other moves in the beginning of the algorithm,\n",
    "    # but exploitation later for a greater reward.\n",
    "    #prob[Q_state_index][best_action] = prob[Q_state_index][best_action] + ((1 - epsilon) * (1 - prob[Q_state_index][best_action]))\n",
    "    prob[Q_state_index][best_action] = min(1, prob[Q_state_index][best_action] + 1 - epsilon)\n",
    "    \n",
    "    other_action = 1 if best_action == 0 else 0\n",
    "    prob[Q_state_index][other_action] = 1 - prob[Q_state_index][best_action]\n",
    "    \n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b1af7c87-1a2f-4fd7-ace1-ed8d1ed4e632",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 193 is out of bounds for axis 0 with size 180",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5324/917225351.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mnew_Q\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_mc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mend_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5324/1591971923.py\u001b[0m in \u001b[0;36mrun_mc\u001b[1;34m(env, num_episodes)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mepisode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplay_game\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mepsilon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdecay\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon_min\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5324/3151758207.py\u001b[0m in \u001b[0;36mplay_game\u001b[1;34m(env, Q, prob)\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[1;31m# to get the index of the action that has the highest current Q\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[1;31m# value. Index 0 is hit, index 1 is stand.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[0mbest_action\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mQ_state_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;31m# Go to the prob table to retrieve the probability of this action.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 193 is out of bounds for axis 0 with size 180"
     ]
    }
   ],
   "source": [
    "import time\n",
    "env = BlackjackEnv()\n",
    "\n",
    "start_time = time.time()\n",
    "new_Q, new_prob = run_mc(env, 1000000)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Total Time for Learning: \" + str(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "36124659-b666-4ad5-873b-6ce84bcde83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_policy(Q):\n",
    "    best_policy_binary = []\n",
    "    best_policy_string = []\n",
    "    best_policy_colors = []\n",
    "    for i in range(len(Q)):\n",
    "        best_policy_binary.append(np.argmax(Q[i]))\n",
    "        best_policy_string.append(\"H\" if np.argmax(Q[i]) == 0 else \"S\")\n",
    "        best_policy_colors.append(\"g\" if np.argmax(Q[i]) == 0 else \"r\")\n",
    "        \n",
    "    return best_policy_binary, best_policy_string, best_policy_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0b2eb594-cbcd-47b6-b8d3-a97557906e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "new_Q_binary, new_Q_string, new_Q_colors = best_policy(new_Q)\n",
    "\n",
    "df = pd.DataFrame(columns = range(2, 12))\n",
    "\n",
    "color_df = pd.DataFrame(columns = range(2, 12))\n",
    "\n",
    "for s in range(3, 21): # possible player values in the range 3 to 20\n",
    "    start = env.observation_space[1].n * (s-3)\n",
    "    end = start + 10\n",
    "    df.loc[s]=(new_Q_string[start:end])\n",
    "    color_df.loc[s]=(new_Q_colors[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cfd8c987-fba4-442e-933a-033b702c4a4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# hide axes\n",
    "fig.patch.set_visible(False)\n",
    "ax.set_axis_off()\n",
    "ax.axis('tight')\n",
    "\n",
    "ax.table(cellText=df.values, cellColours=color_df.values, cellLoc=\"center\", rowLabels=df.index, colLabels=df.columns, loc='center')\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1cca425e-ac93-4bed-a0c1-e6112c63c87c",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5324/441046385.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0mQ_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_Q_state_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_Q_binary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mQ_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mnew_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import random\n",
    "env = BlackjackEnv()\n",
    "\n",
    "total_rewards = 0\n",
    "NUM_EPISODES = 100000\n",
    "\n",
    "for _ in range(NUM_EPISODES):\n",
    "    state = env.reset()\n",
    "\n",
    "    while env.done == False:\n",
    "        if state[0] == 19: # Player was dealt Blackjack\n",
    "            next_state, reward, env.done, info = env.step(1) # doesn't matter what action is taken.\n",
    "            # don't do any episode analysis for this episode. This is a useless episode.\n",
    "            total_rewards += reward\n",
    "        else:\n",
    "            Q_index = get_Q_state_index(state)\n",
    "            action = new_Q_binary[Q_index]\n",
    "\n",
    "            new_state, reward, done, desc = env.step(action)\n",
    "            state = new_state\n",
    "            total_rewards += reward\n",
    "        \n",
    "avg_reward = total_rewards / NUM_EPISODES\n",
    "print(avg_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5460cb89-95bb-48e8-add2-543a9bc851e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
